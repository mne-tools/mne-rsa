{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Tutorial part 1: RSA on sensor-level MEG data\nIn this tutorial, we will perform sensor-level RSA analysis on MEG data.\n\nWe will explore how representational similarity analysis (RSA) can be used to study the\nneural representational code within visual cortex. We will start with performing RSA on\nthe sensor level data, followed by source level and finally we will perform group level\nstatistical analysis. Along the way, we will encounter many of the functions and classes\noffered by MNE-RSA, which will always be presented in the form of links to the\n`api_documentation` which you are encouraged to explore.\n\nThe dataset we will be working with today is the MEG data of the [Wakeman & Nelson\n(2015) \u201cfaces\u201d dataset](https://www.nature.com/articles/sdata20151)_. During this\nexperiment, participants were presented with a series of images, containing:\n\n- Faces of famous people that the participants likely knew\n- Faces of people that the participants likely did not know\n- Scrambled faces: the images were cut-up and randomly put together again\n\nAs a first step, you need to download and extract the dataset: [rsa-data.zip](https://github.com/wmvanvliet/neuroscience_tutorials/releases/download/2/rsa-data.zip)_.\nYou can either do this by executing the cell below, or you can do so manually. In any\ncase, make sure that the ``data_path`` variable points to where you have extracted the\n[rsa-data.zip](https://github.com/wmvanvliet/neuroscience_tutorials/releases/download/2/rsa-data.zip)_\nfile to.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# ruff: noqa: E402\n# sphinx_gallery_thumbnail_number=8\n\nimport os\n\nimport pooch\n\n# Download and unzip the data\npooch.retrieve(\n    url=\"https://github.com/wmvanvliet/neuroscience_tutorials/releases/download/2/rsa-data.zip\",\n    known_hash=\"md5:859c0684dd25f8b82d011840725cbef6\",\n    progressbar=True,\n    processor=pooch.Unzip(members=[\"data\"], extract_dir=os.getcwd()),\n)\n# Set this to where you've extracted `rsa-data.zip` to\ndata_path = \"data\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# A representational code for the stimuli\n\nLet\u2019s start by taking a look at the stimuli that were presented during the experiment.\nThey reside in the ``stimuli`` folder for you as ``.bmp`` image files. The Python\nImaging Library (PIL) can open them and we can use matplotlib to display them.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nfrom PIL import Image\n\n# Show the first \"famous\" face and the first \"scrambled\" face\nimg_famous = Image.open(f\"{data_path}/stimuli/f001.bmp\")\nimg_scrambled = Image.open(f\"{data_path}/stimuli/s001.bmp\")\n\nfig, axes = plt.subplots(1, 2, figsize=(10, 5))\naxes[0].imshow(img_famous, cmap=\"gray\")\naxes[0].set_title(f\"Famous face: {img_famous.width} x {img_famous.height} pixels\")\naxes[1].imshow(img_scrambled, cmap=\"gray\")\naxes[1].set_title(\n    f\"Scrambled face: {img_scrambled.width} x {img_scrambled.height} pixels\"\n)\naxes[0].axis(\"off\")\naxes[1].axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Loaded like this, the stimuli are in a representational space defined by their pixels.\nEach image is represented by 128 x 162 = 20736 values between 0 (black) and 255\n(white). Let's create a Representational Dissimilarity Matrix (RDM) where images are\ncompared based on the difference between their pixels. To get the pixels of an image,\nyou can convert it to a NumPy array like this:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\npixels_famous = np.array(img_famous)\npixels_scrambled = np.array(img_scrambled)\n\nprint(\"Shape of the pixel array for the famous face:\", pixels_famous.shape)\nprint(\"Shape of the pixel array for the scrambled face:\", pixels_scrambled.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now compute the \u201cdissimilarity\u201d between the two images, based on their pixels.\nFor this, we need to decide on a metric to use. The default metric used in the\noriginal publication ([Kiegeskorte et al.\u00a02008](https://www.frontiersin.org/articles/10.3389/neuro.06.004.2008/full)_)\nwas Pearson Correlation, so let\u2019s use that. Of course, correlation is a\nmetric of similarity and we want a metric of *dis*\\ similarity. Let\u2019s make it easy on\nourselves and just do $1 - r$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from scipy.stats import pearsonr\n\nsimilarity, _ = pearsonr(pixels_famous.flatten(), pixels_scrambled.flatten())\ndissimilarity = 1 - similarity\nprint(\n    \"The dissimilarity between the pixels of the famous and scrambled faces is:\",\n    dissimilarity,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To construct the full RDM, we need to do this for all pairs of images. In the cell\nbelow, we make a list of all image files and load all of them (there are 450), convert\nthem to NumPy arrays and concatenate them all together in a single big array called\n``pixels`` of shape ``n_images x width x height``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from glob import glob\n\nfiles = sorted(glob(f\"{data_path}/stimuli/*.bmp\"))\nprint(f\"There are {len(files)} images to read.\")\n\npixels = np.array([np.array(Image.open(f)) for f in files])\nprint(\"The dimensions of the `pixel` array are:\", pixels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Your first RDM\n\nNow that you have all the images loaded in, computing the pairwise dissimilarities is\na matter of looping over them and computing correlations. We could do this manually,\nbut we can make our life a lot easier by using MNE-RSA\u2019s :func:`mne_rsa.compute_rdm`\nfunction. It wants the big matrix as input and also takes a ``metric`` parameter to\nselect which dissimilarity metric to use. Setting it to ``metric=\"correlation\"``,\nwhich is also the default by the way, will make it use (1 - Pearson correlation) as a\nmetric like we did manually above.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from mne_rsa import compute_rdm, plot_rdms\n\npixel_rdm = compute_rdm(pixels)\nplot_rdms(pixel_rdm, names=\"pixels\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Staring deeply into this RDM will reveal to you which images belonged to the\n\u201cscrambled faces\u201d class, as those pixels are quite different from the actual faces and\neach other. We also see that for some reason, the famous faces are a little more alike\nthan the unknown faces.\n\nThe RDM is symmetric along the diagonal, which is all zeros. Take a moment to ponder\nwhy that would be.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>The :func:`mne_rsa.compute_rdm` function is a wrapper around\n   :func:`scipy.spatial.distance.pdist`. This means that all the metrics supported by\n   :func:`~scipy.spatial.distance.pdist` are also valid for\n   :func:`mne_rsa.compute_rdm`. This also means that in MNE-RSA, the native format for\n   an RDM is the so-called \"condensed\" form. Since RDMs are symmetric, only the upper\n   triangle is stored. The :func:`scipy.spatial.distance.squareform` function can be\n   used to go from a square matrix to its condensed form and back.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Your second RDM\n\nThere are many sensible representations possible for images. One intriguing one is to\ncreate them using convolutional neural networks (CNNs). For example, there is the\n[FaceNet](https://github.com/davidsandberg/facenet)_ model by [Schroff et al.\u00a0(2015)](http://arxiv.org/abs/1503.03832)_ that can generate high-level representations,\nsuch that different photos of the same face have similar representations. I have run\nthe stimulus images through FaceNet and recorded the generated embeddings for you to\nuse:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "store = np.load(f\"{data_path}/stimuli/facenet_embeddings.npz\")\nfilenames = store[\"filenames\"]\nembeddings = store[\"embeddings\"]\nprint(\n    \"For each of the 450 images, the embedding is a vector of length 512:\",\n    embeddings.shape,\n)\n\nfacenet_rdm = compute_rdm(embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets plot both RDMs side-by-side:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_rdms([pixel_rdm, facenet_rdm], names=[\"pixels\", \"facenet\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# A look at the brain data\n\nWe\u2019ve seen how we can create RDMs using properties of the images or embeddings\ngenerated by a model. Now it\u2019s time to see how we create RDMs based on the MEG data.\nFor that, we first load the epochs from a single participant.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import mne\n\nepochs = mne.read_epochs(f\"{data_path}/sub-02/sub-02-epo.fif\")\nepochs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each epoch corresponds to the presentation of an image, and the signal across the\nsensors over time can be used as the neural representation of that image. Hence, one\ncould make a neural RDM, of for example the gradiometers in the time window 100 to 200\nms after stimulus onset, like this:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "neural_rdm = compute_rdm(epochs.copy().pick(\"grad\").crop(0.1, 0.2).get_data())\nplot_rdms(neural_rdm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To compute RSA scores, we want to compare the resulting neural RDM with the RDMs we\u2019ve\ncreated earlier. However, if we inspect the neural RDM closely, we see that its rows\nand column don\u2019t line up with those of the previous RDMs. There are too many (879\nvs.\u00a0450) and they are in the wrong order. Making sure that the RDMs match is an\nimportant and sometimes tricky part of RSA.\n\nTo help us out, a useful feature of MNE-Python is that epochs have an associated\n:attr:`mne.Epochs.metadata` field. This metadata is a :class:`pandas.DataFrame` where\neach row contains information about the corresponding epoch. The epochs in this\ntutorial come with some useful ``.metadata`` already:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "epochs.metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "While the trigger codes only indicate what type of stimulus was shown, the ``file``\ncolumn of the metadata tells us the exact image. Couple of challenges here: the\nstimuli where shown in a random order, stimuli were repeated twice during the\nexperiment, and some epochs were dropped during preprocessing so not every image is\nnecessarily present twice in the ``epochs`` data. \ud83d\ude29\n\nLuckily, MNE-RSA has a way to make our lives easier. Let\u2019s take a look at the\n:func:`mne_rsa.rdm_epochs` function, the Swiss army knife for computing RDMs from an\nMNE-Python :class:`mne.Epochs` object.\n\nIn MNE-Python tradition, the function has a lot of parameters, but\nall-but-one have a default so you only have to specify the ones that are\nrelevant to you. For example, to redo the neural RDM we created above,\nwe could do something like:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from mne_rsa import rdm_epochs\n\nneural_rdm_gen = rdm_epochs(epochs, tmin=0.1, tmax=0.2)\n\n# `rdm_epochs` returns a generator of RDMs\n# unpacking the first (and only) RDM from the generator\nneural_rdm = next(neural_rdm_gen)\nplot_rdms(neural_rdm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Take note that :func:`mne_rsa.rdm_epochs` returns a [generator](https://wiki.python.org/moin/Generators)_ of RDMs. This is because one of the main\nuse-cases for MNE-RSA is to produce RDMs using sliding windows (in time and also in\nspace), which can produce a large amount of RDMs that can take up a lot of memory of\nyou\u2019re not careful.\n\n# Alignment between model and data RDM ordering\n\nLooking at the neural RDM above, something is clearly different from the\none we made before. This one has 9 rows and columns. Closely inspecting\nthe docstring of :class:`mne_rsa.rdm_epochs` reveals that it is the ``labels``\nparameter that is responsible for this:\n\n::\n\n  labels : list | None\n      For each epoch, a label that identifies the item to which it corresponds.\n      Multiple epochs may correspond to the same item, in which case they should have\n      the same label and will either be averaged when computing the data RDM\n      (``n_folds=1``) or used for cross-validation (``n_folds>1``). Labels may be of\n      any python type that can be compared with ``==`` (int, float, string, tuple,\n      etc). By default (``None``), the epochs event codes are used as labels.\n\nInstead of producing one row per epoch, :func:`mne_rsa.rdm_epochs` produced one row\nper event type, averaging across epochs of the same type before computing\ndissimilarity. This is not quite what we want though. If we want to match\n``pixel_rdm`` and ``facenet_rdm``, we want every single one of the 450 images to be\nits own stimulus type. We can achieve this by setting the ``labels`` parameter of\n:func:`mne_rsa.rdm_epochs` to a list that assigns each of the 879 epochs to a label\nthat indicates which image was shown. An image is identified by its filename, and the\n``epochs.metadata.file`` column contains the filenames corresponding to the epochs,\nso let's use that.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "neural_rdm = next(rdm_epochs(epochs, labels=epochs.metadata.file, tmin=0.1, tmax=0.2))\n\n# This plots your RDM\nplot_rdms(neural_rdm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The cell below will compure RSA between the neural RDM and the pixel and FaceNet RDMs\nwe created earlier. The RSA score will be the Spearman correlation between the RDMs,\nwhich is the default metric used in the [original RSA paper](https://www.frontiersin.org/articles/10.3389/neuro.06.004.2008/full)_.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from mne_rsa import rsa\n\nrsa_pixel = rsa(neural_rdm, pixel_rdm, metric=\"spearman\")\nrsa_facenet = rsa(neural_rdm, facenet_rdm, metric=\"spearman\")\n\nprint(\"RSA score between neural RDM and pixel RDM:\", rsa_pixel)\nprint(\"RSA score between neural RDM and FaceNet RDM:\", rsa_facenet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Slippin\u2019 and slidin\u2019 across time\n\nThe neural representation of a stimulus is different across brain\nregions and evolves over time. For example, we would expect that the\npixel RDM would be more similar to a neural RDM that we computed across\nthe visual cortex at an early time point, and that the FaceNET RDM might\nbe more similar to a neural RDM that we computed at a later time point.\n\nFor the remainder of this tutorial, we\u2019ll restrict the ``epochs`` to\nonly contain the sensors over the left occipital cortex.\n\n<div class=\"alert alert-danger\"><h4>Warning</h4><p>Just because we select sensors over a certain brain region, does not mean the\n   magnetic fields originate from that region. This is especially true for\n   magnetometers. To make it a bit more accurate, we only select gradiometers.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "picks = mne.channels.read_vectorview_selection(\"Left-occipital\")\npicks = [\"\".join(p.split(\" \")) for p in picks]\nepochs.pick(picks).pick(\"grad\").crop(-0.1, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the cell below, we use :func:`mne_rsa.rdm_epochs` to compute RDMs using a sliding\nwindow by setting the ``temporal_radius`` parameter to ``0.1`` seconds. We use the\nentire time range (``tmin=None`` and ``tmax=None``) and leave the result as a\ngenerator (so no ``next()`` calls).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "neural_rdms_gen = rdm_epochs(epochs, labels=epochs.metadata.file, temporal_radius=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And now we can consume the generator (with a nice progress bar) and plot\na few of the generated RDMs:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n\ntimes = epochs.times[(epochs.times >= 0) & (epochs.times <= 0.9)]\nneural_rdms_list = list(tqdm(neural_rdms_gen, total=len(times)))\nplot_rdms(neural_rdms_list[::10], names=[f\"t={t:.2f}\" for t in times[::10]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Putting it altogether for sensor-level RSA\n\nNow all that is left to do is compute RSA scored between the neural RDMs you\u2019ve just\ncreated and the pixel and FaceNet RDMs. We could do this using the\n:func:`mne_rsa.rsa_gen` function, but I\u2019d rather directly show you the\n:func:`mne_rsa.rsa_epochs` function that combines computing the neural RDMs with\ncomputing the RSA scores.\n\nThe signature of :func:`mne_rsa.rsa_epochs` is very similar to that of\n:func:`mne_rsa.rdm_epochs` The main difference is that we also give it the \u201cmodel\u201d\nRDMs, in our case the pixel and FaceNet RDMs. We can also specify ``labels_rdm_model``\nto indicate which rows of the model RDMs correspond to which images to make sure the\nordering is the same. :func:`mne_rsa.rsa_epochs` will return the RSA scores as a list\nof :class:`mne.Evoked` objects: one for each model RDM we gave it.\n\nWe compute the RSA scores for ``epochs`` against ``[pixel_rdm, facenet_rdm]`` and do\nthis in a sliding windows across time, with a temporal radius of 0.1 seconds. Setting\n``verbose=True`` will activate a progress bar. We can optionally set ``n_jobs=-1`` to\nuse multiple CPU cores to speed things up.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from mne_rsa import rsa_epochs\n\nev_rsa = rsa_epochs(\n    epochs,\n    [pixel_rdm, facenet_rdm],\n    labels_epochs=epochs.metadata.file,\n    labels_rdm_model=filenames,\n    temporal_radius=0.1,\n    verbose=True,\n    n_jobs=-1,\n)\n\n# Create a nice plot of the result\nev_rsa[0].comment = \"pixels\"\nev_rsa[1].comment = \"facenet\"\nmne.viz.plot_compare_evokeds(\n    ev_rsa, picks=[0], ylim=dict(misc=[-0.02, 0.2]), show_sensors=False\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that first, the \u201cpixels\u201d representation is the better match to the\nrepresentation in the brain, but after around 150 ms the representation produced by\nthe FaceNet model matches better. The best match between the brain and FaceNet is\nfound at around 250 ms.\n\nIf you\u2019ve made it this far, you have successfully completed your first sensor-level\nRSA! \ud83c\udf89 This is the end of this tutorial. I invite you to join me in the next\ntutorial where we will do source level RSA: `tut-source-level`\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}