{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Sensor-level RSA using mixed sensor types\n\nThis example demonstrates how to perform representational similarity analysis (RSA) on\nMEEG data containing magnetometers, gradiometers and EEG channels. In this scenario\nthere are important things we need to keep in mind:\n\n1. Different sensor types see the underlying sources from different perspectives, hence\n   spatial searchlight patches based on the sensor positions are a bad idea. We will\n   perform a searchlight over time only, pooling data from all sensors at all times.\n2. The sensors have different units of measurement, hence the numeric data is in\n   different orders of magnitude. If we don't compensate for this, only the sensors with\n   data in the highest order of magnitude will matter when compuring RDMs. We will\n   compute a noise covariance matrix and perform data whitening to achieve this.\n\nThe dataset will be the MNE-sample dataset: a collection of 288 epochs in which the\nparticipant was presented with an auditory beep or visual stimulus to either the left or\nright ear or visual field.\n\n| Authors:\n| Marijn van Vliet <marijn.vanvliet@aalto.fi>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# sphinx_gallery_thumbnail_number=2\n\n# Import required packages\nimport operator\n\nimport mne\nimport mne_rsa\nimport numpy as np\n\nmne.set_log_level(False)  # Be less verbose"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll be using the data from the MNE-sample set.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sample_root = mne.datasets.sample.data_path(verbose=True)\nsample_path = sample_root / \"MEG\" / \"sample\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creating epochs from the continuous (raw) data. We downsample to 100 Hz to speed up\nthe RSA computations later on.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "raw = mne.io.read_raw_fif(sample_path / \"sample_audvis_filt-0-40_raw.fif\")\nevents = mne.read_events(sample_path / \"sample_audvis_filt-0-40_raw-eve.fif\")\nevent_id = {\"audio/left\": 1, \"visual/left\": 3}\nepochs = mne.Epochs(raw, events, event_id, preload=True)\nepochs.resample(100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plotting the evokeds for each sensor type. Not the difference in scaling of the values\n(=the y-limits of the plot).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "epochs.average().plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To estimate the differences in signal amplitude between the different sensor types, we\ncompute the (co-)variance during a period of relative rest in the signal: the baseline\nperiod (-200 to 0 milliseconds). See [MNE-Python's covariance tutorial](https://mne.tools/stable/auto_tutorials/forward/90_compute_covariance.html)_ for\ndetails.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "noise_cov = mne.compute_covariance(\n    epochs, tmin=-0.2, tmax=0, method=\"shrunk\", rank=\"info\"\n)\nnoise_cov.plot(epochs.info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we compute a reference RDM (simply encoding visual vs audio condition) and RSA it\nagainst the sensor data, which we will do in a sliding window across time.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Sort the epochs by condition\nepochs = mne.concatenate_epochs([epochs[\"audio\"], epochs[\"visual\"]])\n\n# Compute model RDM\nmodel_rdm = mne_rsa.compute_rdm(epochs.events[:, 2], metric=operator.ne)\nmne_rsa.plot_rdms(model_rdm)\n\n# Perform RSA across time\nrsa_scores = mne_rsa.rsa_epochs(\n    epochs,\n    model_rdm,\n    noise_cov=noise_cov,\n    temporal_radius=0.02,\n    y=np.arange(len(epochs)),\n)\nrsa_scores.plot(units=dict(misc=\"Spearman correlation\"))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}