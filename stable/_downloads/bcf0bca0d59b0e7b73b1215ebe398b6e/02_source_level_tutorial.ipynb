{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Tutorial part 2: RSA on source-level MEG data\n\nIn this tutorial, we will perform source-level RSA analysis on MEG data.\n\nIf you haven't done so, I recommend first completing `tut-sensor-level`.\n\nWhile sensor-level RSA is useful to get a first impression of how neural representations\nunfold over time, it is not really suited to study differences between brain regions.\nFor this, you want to so RSA in a searchlight pattern across the cortex.\n\nThe knowledge you have gained from your sensor-level analysis will serve you well for\nthis part, as the API of MNE-RSA is mostly the same across sensor- and source-level\nanalysis. However, performing a searchlight analysis is a heavy computation that can\ntake a lot of time. Hence, we will also learn about the API regarding restricting the\nanalysis to parts of the data in several ways.\n\nIn the cell below, update the ``data_path`` variable to point to where you have\nextracted the [rsa-data.zip](https://github.com/wmvanvliet/neuroscience_tutorials/releases/download/2/rsa-data.zip)_\nfile to.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# ruff: noqa: E402\n# sphinx_gallery_thumbnail_number=3\n\n# Set this to where you've extracted `data.zip` to\ndata_path = \"data\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We\u2019ll start by loading the ``epochs`` again, but this time, we will restrict them to\nonly two experimental conditions: the first presentations of famous faces versus\nscrambled faces. This will reduce the number of rows/columns in our RDMs and hence\nspeed up computing and comparing them.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import mne\n\nepochs = mne.read_epochs(f\"{data_path}/sub-02/sub-02-epo.fif\")\nepochs = epochs[[\"face/famous/first\", \"scrambled/first\"]]\nepochs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When we select a subset of epochs, the ``epochs.metadata`` field is likewise updated\nto match the new selection. This feature is one of the main reasons to use the\n``.metadata`` field instead of keeping a separate :class:`pandas.DataFrame` manually.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "epochs.metadata.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This means we can use the ``epochs.metadata[\"file\"]`` column to restrict the pixel and\nFaceNet RDMs to just those images still present in the MEG data.\n\nIn the cell below, we read the images and FaceNet embeddings and select the proper\nrows from the data matrices and use ``compute_dsm`` to compute the appropriate RDMs.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from glob import glob\n\nimport numpy as np\nfrom PIL import Image\n\nfiles = sorted(glob(f\"{data_path}/stimuli/*.bmp\"))\npixels = np.array([np.array(Image.open(f)) for f in files])\n\nstore = np.load(f\"{data_path}/stimuli/facenet_embeddings.npz\")\nfilenames = store[\"filenames\"]\nembeddings = store[\"embeddings\"]\n\n# Select the proper filenames\nepochs_filenames = set(epochs.metadata[\"file\"])\nselection = [f in epochs_filenames for f in filenames]\nfilenames = filenames[selection]\n\n# Select the proper rows from `pixels` and `embeddings` and compute the RDMs.\nfrom mne_rsa import compute_rdm\n\npixel_rdm = compute_rdm(pixels[selection])\nfacenet_rdm = compute_rdm(embeddings[selection])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Executing the cell below will test whether the RDMs have been properly constructed and\nplot them.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from mne_rsa import plot_rdms\nfrom scipy.spatial.distance import squareform\n\nif len(pixel_rdm) != len(facenet_rdm):\n    print(\"The pixel and FaceNet RDMs are of difference sizes, that can't be right. \ud83e\udd14\")\nelif len(pixel_rdm) != 43956:\n    print(\"Looks like the RDMs do not have the correct rows. \ud83e\udd14\")\nelif (\n    squareform(pixel_rdm)[:150, :150].mean() >= squareform(pixel_rdm)[150:, 150:].mean()\n):\n    print(\n        \"The pixels RDM doesn't look quite right. Make sure the rows are in \"\n        \"alphabetical filename order. \ud83e\udd14\"\n    )\nelif (\n    squareform(facenet_rdm)[:150, :150].mean()\n    <= squareform(facenet_rdm)[150:, 150:].mean()\n):\n    print(\n        \"The FaceNet RDM doesn't look quite right. Make sure the rows are in \"\n        \"alphabetical filename order. \ud83e\udd14\"\n    )\nelse:\n    print(\"The RDMs look just right! \ud83d\ude0a\")\n    plot_rdms([pixel_rdm, facenet_rdm], names=[\"pixels\", \"facenet\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# To source space!\n\nIn order to perform RSA in source space, we must create source estimates for the\nepochs. There\u2019s many different ways to do this, for example you\u2019ll learn about\nbeamformers during this workshop, but here we\u2019re going to use the one that is fastest.\nIf we use MNE, we can use a pre-computed inverse operator and apply it to the epochs\nto quickly get source estimates.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from mne.minimum_norm import apply_inverse_epochs, read_inverse_operator\n\ninv = read_inverse_operator(f\"{data_path}/sub-02/sub-02-inv.fif\")\nstcs = apply_inverse_epochs(epochs, inv, lambda2=1 / 9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The result is a list of 297 ``SourceEstimate`` objects. Here are the\nfirst 5 of them:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "stcs[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The plan is to perform RSA in a searchlight pattern, not only as a sliding window\nthrough time, but also sliding across different locations across the cortex. To this\nend, we\u2019ll define spatial patches with a certain radius, and only source points that\nfall within a patch are taken into account when computing the RDM for that patch. The\ncortex is heavily folded and ideally we define distances between source point as the\nshortest path along the cortex, what is known as the geodesic distance, rather than\nstraight euclidean distance between the XYZ coordinates. MNE-Python is here to help us\nout in this regard, as it contains a function to compute such distances and store them\nwithin the :class:`mne.SourceSpaces` object (through the\n:func:`mne.add_source_space_distances` function).\n\nLet\u2019s load the file containing the proper source space with pre-computed geodesic\ndistances between source points:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from mne import read_source_spaces\n\nsrc = read_source_spaces(f\"{data_path}/freesurfer/sub-02/bem/sub-02-oct5-src.fif\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To speed things up, let\u2019s restrict the analysis to only the occipital, parietal and\ntemporal areas on the left hemisphere. There are several ways to tell MNE-RSA which\nsource points to use, and one of the most convenient ones is to use :class:`mne.Label`\nobjects. This allows us to define the desired areas using the \u201caparc\u201d atlas that\nFreeSurfer has constructed for us:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "rois = mne.read_labels_from_annot(\n    \"sub-02\", parc=\"aparc\", subjects_dir=f\"{data_path}/freesurfer\", hemi=\"lh\"\n)\n\n# These are the regions we're interested in\nroi_sel = [\n    \"inferiortemporal-lh\",\n    \"middletemporal-lh\",\n    \"fusiform-lh\",\n    \"bankssts-lh\",\n    \"inferiorparietal-lh\",\n    \"lateraloccipital-lh\",\n    \"lingual-lh\",\n    \"pericalcarine-lh\",\n    \"cuneus-lh\",\n    \"supramarginal-lh\",\n    \"superiorparietal-lh\",\n]\nrois = [r for r in rois if r.name in roi_sel]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Source-space RSA\n\nTime to actually perform the RSA in source space. The function you need is\n:func:`mne_rsa.rsa_stcs` Take a look at the documentation of that function, which\nshould look familiar is it is very similar to :func:`mne_rsa.rsa_epochs` that you have\nused before.\n\nWe will perform RSA on the source estimates, using the pixel and FaceNet RDMs as model\nRDMs. As was the case with the sensor-level RSA, we will need specify labels to\nindicate which image was shown during which epoch and which image corresponds to each\nrow/column of the ``pixel_rdm`` and ``facenet_rdm``. We will use the filenames for\nthis.\n\nSearchlight patches swill have a spatial radius of 2cm (=0.02 meters) and a\ntemporal radius of 50 ms (=0.05 seconds). We will restrict the analysis to 0.0 to 0.5\nseconds after stimulus onset and to the cortical regions (``rois``) we\u2019ve selected\nabove. We can optionally set ``n_jobs=-1`` to use all CPU cores and ``verbose=True``\nto show a progress bar.\n\nDepending on the speed of your computer, this may take anywhere from a few seconds to\na few minutes to complete.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from mne_rsa import rsa_stcs\n\nstc_rsa = rsa_stcs(\n    stcs,\n    [pixel_rdm, facenet_rdm],\n    src=src,\n    labels_stcs=epochs.metadata.file,\n    labels_rdm_model=filenames,\n    tmin=0,\n    tmax=0.5,\n    sel_vertices=rois,\n    spatial_radius=0.02,\n    temporal_radius=0.05,\n    verbose=True,\n    n_jobs=-1,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If everything went as planned, executing the cell below will plot the result.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# For clarity, only show positive RSA scores\nstc_rsa[0].data[stc_rsa[0].data < 0] = 0\nstc_rsa[1].data[stc_rsa[1].data < 0] = 0\n\n# Show the RSA maps for both the pixel and FaceNet RDMs\nbrain_pixels = stc_rsa[0].plot(\n    \"sub-02\",\n    subjects_dir=f\"{data_path}/freesurfer\",\n    hemi=\"both\",\n    initial_time=0.081,\n    views=\"ventral\",\n    title=\"pixels\",\n)\nbrain_facenet = stc_rsa[1].plot(\n    \"sub-02\",\n    subjects_dir=f\"{data_path}/freesurfer\",\n    hemi=\"both\",\n    initial_time=0.180,\n    views=\"parietal\",\n    title=\"FaceNet\",\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you\u2019ve made it this far, you have successfully completed your first sensor-level\nRSA! \ud83c\udf89 This is the end of this tutorial. In the next tutorial, we will discuss\ngroup-level analysis and statistics: `tut-statistics`.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}