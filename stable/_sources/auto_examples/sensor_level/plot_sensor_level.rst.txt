
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/sensor_level/plot_sensor_level.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_sensor_level_plot_sensor_level.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_sensor_level_plot_sensor_level.py:


Sensor-level RSA using a searchlight
------------------------------------

This example demonstrates how to perform representational similarity analysis (RSA) on
EEG data, using a searchlight approach.

In the searchlight approach, representational similarity is computed between the model
and searchlight "patches". A patch is defined by a seed point (e.g. sensor Pz) and
everything within the given radius (e.g. all sensors within 4 cm. of Pz). Patches are
created for all possible seed points (e.g. all sensors), so you can think of it as a
"searchlight" that moves from seed point to seed point and everything that is in the
spotlight is used in the computation.

The radius of a searchlight can be defined in space, in time, or both. In this example,
our searchlight will have a spatial radius of 4.5 cm. and a temporal radius of 50 ms.

..warning::
    A searchlight across MEG sensors does not make a lot of sense, as the magnetic field
    pattern does not lend itself well to circular searchlight patches.

The dataset will be the kiloword dataset [1]_: approximately 1,000 words were presented
to 75 participants in a go/no-go lexical decision task while event-related potentials
(ERPs) were recorded.

.. [1] Dufau, S., Grainger, J., Midgley, KJ., Holcomb, PJ (2015). A thousand words are
       worth a picture: Snapshots of printed-word processing in an event-related
       potential megastudy. Psychological science.

| Authors:
| Marijn van Vliet <marijn.vanvliet@aalto.fi>

.. GENERATED FROM PYTHON SOURCE LINES 36-42

.. code-block:: Python

    # sphinx_gallery_thumbnail_number=2

    # Import required packages
    import mne
    import mne_rsa








.. GENERATED FROM PYTHON SOURCE LINES 43-48

MNE-Python contains a built-in data loader for the kiloword dataset. We use it here to
read it as 960 epochs. Each epoch represents the brain response to a single word,
averaged across all the participants. For this example, we speed up the computation,
at a cost of temporal precision, by downsampling the data from the original 250 Hz. to
100 Hz.

.. GENERATED FROM PYTHON SOURCE LINES 48-53

.. code-block:: Python


    data_path = mne.datasets.kiloword.data_path(verbose=True)
    epochs = mne.read_epochs(data_path / "kword_metadata-epo.fif")
    epochs = epochs.resample(100)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Reading /home/runner/mne_data/MNE-kiloword-data/kword_metadata-epo.fif ...
    Isotrak not found
        Found the data of interest:
            t =    -100.00 ...     920.00 ms
            0 CTF compensation matrices available
    Adding metadata with 8 columns
    960 matching events found
    No baseline correction applied
    0 projection items activated




.. GENERATED FROM PYTHON SOURCE LINES 54-57

The kiloword dataset was erroneously stored with sensor locations given in centimeters
instead of meters. We will fix it now. For your own data, the sensor locations are
likely properly stored in meters, so you can skip this step.

.. GENERATED FROM PYTHON SOURCE LINES 57-61

.. code-block:: Python

    for ch in epochs.info["chs"]:
        ch["loc"] /= 100









.. GENERATED FROM PYTHON SOURCE LINES 62-65

The ``epochs`` object contains a ``.metadata`` field that contains information about
the 960 words that were used in the experiment. Let's have a look at the metadata for
10 random words:

.. GENERATED FROM PYTHON SOURCE LINES 65-68

.. code-block:: Python


    epochs.metadata.sample(10)






.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>WORD</th>
          <th>Concreteness</th>
          <th>WordFrequency</th>
          <th>OrthographicDistance</th>
          <th>NumberOfLetters</th>
          <th>BigramFrequency</th>
          <th>ConsonantVowelProportion</th>
          <th>VisualComplexity</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>716</th>
          <td>emissary</td>
          <td>4.941176</td>
          <td>1.301030</td>
          <td>3.35</td>
          <td>8.0</td>
          <td>243.500000</td>
          <td>0.500000</td>
          <td>67.482950</td>
        </tr>
        <tr>
          <th>27</th>
          <td>gaze</td>
          <td>3.800000</td>
          <td>2.432969</td>
          <td>1.10</td>
          <td>4.0</td>
          <td>173.000000</td>
          <td>0.500000</td>
          <td>78.553425</td>
        </tr>
        <tr>
          <th>124</th>
          <td>saddle</td>
          <td>6.000000</td>
          <td>2.173186</td>
          <td>1.70</td>
          <td>6.0</td>
          <td>519.666667</td>
          <td>0.666667</td>
          <td>72.100254</td>
        </tr>
        <tr>
          <th>642</th>
          <td>holding</td>
          <td>3.800000</td>
          <td>3.228400</td>
          <td>1.85</td>
          <td>7.0</td>
          <td>867.000000</td>
          <td>0.714286</td>
          <td>66.252681</td>
        </tr>
        <tr>
          <th>350</th>
          <td>forest</td>
          <td>5.950000</td>
          <td>3.086360</td>
          <td>1.90</td>
          <td>6.0</td>
          <td>815.833333</td>
          <td>0.666667</td>
          <td>59.502529</td>
        </tr>
        <tr>
          <th>198</th>
          <td>property</td>
          <td>4.850000</td>
          <td>3.087781</td>
          <td>2.75</td>
          <td>8.0</td>
          <td>882.750000</td>
          <td>0.625000</td>
          <td>64.681534</td>
        </tr>
        <tr>
          <th>806</th>
          <td>sheen</td>
          <td>4.000000</td>
          <td>1.662758</td>
          <td>1.80</td>
          <td>5.0</td>
          <td>592.000000</td>
          <td>0.600000</td>
          <td>72.735896</td>
        </tr>
        <tr>
          <th>218</th>
          <td>prestige</td>
          <td>2.750000</td>
          <td>2.454845</td>
          <td>2.75</td>
          <td>8.0</td>
          <td>724.500000</td>
          <td>0.625000</td>
          <td>67.401006</td>
        </tr>
        <tr>
          <th>220</th>
          <td>abstract</td>
          <td>1.800000</td>
          <td>2.587711</td>
          <td>2.65</td>
          <td>8.0</td>
          <td>481.875000</td>
          <td>0.750000</td>
          <td>63.201725</td>
        </tr>
        <tr>
          <th>598</th>
          <td>senate</td>
          <td>5.550000</td>
          <td>2.418301</td>
          <td>1.95</td>
          <td>6.0</td>
          <td>686.333333</td>
          <td>0.500000</td>
          <td>69.405055</td>
        </tr>
      </tbody>
    </table>
    </div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 69-71

Let's pick something obvious for this example and build a dissimilarity matrix (RDM)
based on the number of letters in each word.

.. GENERATED FROM PYTHON SOURCE LINES 71-75

.. code-block:: Python


    rdm_vis = mne_rsa.compute_rdm(epochs.metadata[["NumberOfLetters"]], metric="euclidean")
    mne_rsa.plot_rdms(rdm_vis)




.. image-sg:: /auto_examples/sensor_level/images/sphx_glr_plot_sensor_level_001.png
   :alt: plot sensor level
   :srcset: /auto_examples/sensor_level/images/sphx_glr_plot_sensor_level_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    <Figure size 200x200 with 2 Axes>



.. GENERATED FROM PYTHON SOURCE LINES 76-81

The above RDM will serve as our "model" RDM. In this example RSA analysis, we are
going to compare the model RDM against RDMs created from the EEG data. The EEG RDMs
will be created using a "searchlight" pattern. We are using squared Euclidean distance
for our RDM metric, since we only have a few data points in each searchlight patch.
Feel free to play around with other metrics.

.. GENERATED FROM PYTHON SOURCE LINES 81-97

.. code-block:: Python


    rsa_result = mne_rsa.rsa_epochs(
        epochs,  # The EEG data
        rdm_vis,  # The model RDM
        epochs_rdm_metric="sqeuclidean",  # Metric to compute the EEG RDMs
        rsa_metric="kendall-tau-a",  # Metric to compare model and EEG RDMs
        spatial_radius=0.05,  # Spatial radius of the searchlight patch in meters.
        temporal_radius=0.05,  # Temporal radius of the searchlight path in seconds.
        tmin=0.15,
        tmax=0.25,  # To save time, only analyze this time interval
        n_jobs=1,  # Only use one CPU core. Increase this for more speed.
        n_folds=None,  # Don't use any cross-validation
        verbose=False,
    )  # Set to True to display a progress bar






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Performing RSA between Epochs and 1 model RDM(s)
        Spatial radius: 0.05 meters
        Using 29 sensors
        Temporal radius: 5 samples
        Time interval: 0.15-0.25 seconds
    Automatic dermination of folds: 1 (no cross-validation)
    Creating spatio-temporal searchlight patches




.. GENERATED FROM PYTHON SOURCE LINES 98-103

The result is packed inside an MNE-Python :class:`mne.Evoked` object. This object
defines many plotting functions, for example :meth:`mne.Evoked.plot_topomap` to look
at the spatial distribution of the RSA values. By default, the signal is assumed to
represent micro-Volts, so we need to explicitly inform the plotting function we are
plotting RSA values and tweak the range of the colormap.

.. GENERATED FROM PYTHON SOURCE LINES 103-114

.. code-block:: Python


    rsa_result.plot_topomap(
        rsa_result.times,
        units=dict(eeg="kendall-tau-a"),
        scalings=dict(eeg=1),
        cbar_fmt="%.4f",
        vlim=(0, None),
        nrows=2,
        sphere=1,
    )




.. image-sg:: /auto_examples/sensor_level/images/sphx_glr_plot_sensor_level_002.png
   :alt: 0.150 s, 0.160 s, 0.170 s, 0.180 s, 0.190 s, 0.200 s, 0.210 s, 0.220 s, 0.230 s, 0.240 s, 0.250 s, AU
   :srcset: /auto_examples/sensor_level/images/sphx_glr_plot_sensor_level_002.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    <MNEFigure size 900x415 with 12 Axes>



.. GENERATED FROM PYTHON SOURCE LINES 115-118

Unsurprisingly, we get the highest correspondence between number of letters and EEG
signal in areas in the `visual word form area
<https://en.wikipedia.org/wiki/Visual_word_form_area>`_.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 30.446 seconds)


.. _sphx_glr_download_auto_examples_sensor_level_plot_sensor_level.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_sensor_level.ipynb <plot_sensor_level.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_sensor_level.py <plot_sensor_level.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_sensor_level.zip <plot_sensor_level.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
